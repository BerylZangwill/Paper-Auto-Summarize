"""
P1.1 æƒé‡è‡ªé€‚åº”è¯„åˆ†ç³»ç»Ÿ - åœºæ™¯é‡ç®—è„šæœ¬

åŠŸèƒ½ï¼š
  1. è¯»å–P0 CSVæ–‡ä»¶ï¼ˆåŒ…å«5ç»´åŸºç¡€è¯„åˆ†ï¼‰
  2. åŠ è½½æƒé‡åœºæ™¯é…ç½®
  3. ä¸ºæ¯ç¯‡è®ºæ–‡è®¡ç®—å¤šä¸ªåœºæ™¯ä¸‹çš„ç»¼åˆè¯„åˆ†
  4. ç”Ÿæˆåˆ†å±‚è¾“å‡ºï¼ˆæ–¹æ¡ˆCï¼‰ï¼š
     - ä¿ç•™åŸP0è¡¨ä¸å˜
     - ç”Ÿæˆå¤šåœºæ™¯å¯¹æ¯”è¡¨
     - ç”ŸæˆæŒ‰å„åœºæ™¯æ’åºçš„æ’åºè¡¨ï¼ˆtop 10é«˜äº®ï¼‰

ä½œè€…: Claude
æ—¥æœŸ: 2026-01-13
"""

import csv
import json
import shutil
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, List, Tuple

# ============================================================
# é…ç½®
# ============================================================
BASE_DIR = Path(__file__).parent
P0_CSV_PATH = BASE_DIR / "02_summary_csv" / "_all_papers.csv"
WEIGHTS_CONFIG_PATH = BASE_DIR / "scenario_weights.json"
OUTPUT_DIR = BASE_DIR / "02_summary_csv"
SCENARIO_RANKINGS_DIR = OUTPUT_DIR / "scenario_rankings"

# æ¨èç­‰çº§æ˜ å°„
SCORE_TO_LEVEL = [
    (9.0, "â­â­â­â­â­"),
    (7.5, "â­â­â­â­"),
    (6.0, "â­â­â­"),
    (4.0, "â­â­"),
    (0.0, "â­")
]

# ============================================================
# æ•°æ®ç»“æ„
# ============================================================
@dataclass
class PaperRecord:
    """è®ºæ–‡è®°å½•"""
    index: str
    title: str
    rigor: float
    innovation: float
    practicality: float
    impact: float
    readability: float
    p0_overall: float
    p0_level: str
    original_row: dict  # ä¿å­˜åŸå§‹è¡Œæ•°æ®ï¼Œç”¨äºè¾“å‡º

@dataclass
class ScenarioScore:
    """åœºæ™¯è¯„åˆ†"""
    scenario_name: str
    overall_score: float
    level: str


# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================
def score_to_level(score: float) -> str:
    """æ ¹æ®è¯„åˆ†æ˜ å°„æ¨èç­‰çº§"""
    for threshold, level in SCORE_TO_LEVEL:
        if score >= threshold:
            return level
    return "â­"


def load_weights_config() -> Dict:
    """åŠ è½½æƒé‡é…ç½®"""
    with open(WEIGHTS_CONFIG_PATH, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_p0_csv() -> List[PaperRecord]:
    """è¯»å–P0 CSVå¹¶è§£æä¸ºPaperRecordå¯¹è±¡"""
    papers = []

    with open(P0_CSV_PATH, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            try:
                paper = PaperRecord(
                    index=row.get('ç¼–å·', '').strip(),
                    title=row.get('æ ‡é¢˜', '').strip(),
                    rigor=float(row.get('å­¦æœ¯ä¸¥è°¨åº¦', 0) or 0),
                    innovation=float(row.get('åˆ›æ–°ç¨‹åº¦', 0) or 0),
                    practicality=float(row.get('å®ç”¨ä»·å€¼', 0) or 0),
                    impact=float(row.get('å½±å“èŒƒå›´', 0) or 0),
                    readability=float(row.get('å¯è¯»æ€§', 0) or 0),
                    p0_overall=float(row.get('ç»¼åˆè¯„åˆ†', 0) or 0),
                    p0_level=row.get('æ¨èç­‰çº§', ''),
                    original_row=row
                )
                papers.append(paper)
            except ValueError as e:
                print(f"âš ï¸  è·³è¿‡ç¬¬{row.get('ç¼–å·', '?')}ç¯‡è®ºæ–‡: è¯„åˆ†æ•°æ®æ ¼å¼é”™è¯¯ - {e}")
                continue

    return papers


def calculate_scenario_score(paper: PaperRecord, weights: Dict) -> ScenarioScore:
    """è®¡ç®—å•ç¯‡è®ºæ–‡åœ¨æŸä¸ªåœºæ™¯ä¸‹çš„ç»¼åˆè¯„åˆ†"""
    overall = (
        paper.rigor * weights['rigor'] +
        paper.innovation * weights['innovation'] +
        paper.practicality * weights['practicality'] +
        paper.impact * weights['impact'] +
        paper.readability * weights['readability']
    )

    # å››èˆäº”å…¥åˆ°ä¸€ä½å°æ•°
    overall = round(overall, 1)
    level = score_to_level(overall)

    return ScenarioScore(
        scenario_name="",
        overall_score=overall,
        level=level
    )


def create_comparison_table(papers: List[PaperRecord], config: Dict, output_path: Path):
    """
    åˆ›å»ºå¤šåœºæ™¯å¯¹æ¯”è¡¨

    åˆ—ç»“æ„ï¼šç¼–å· | æ ‡é¢˜ | 5ç»´åŸå§‹è¯„åˆ† | P0è¯„åˆ†+ç­‰çº§ | å„åœºæ™¯è¯„åˆ†+ç­‰çº§
    """
    scenarios = config['scenarios']

    with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
        # æ„å»ºè¡¨å¤´
        headers = [
            'ç¼–å·', 'æ ‡é¢˜',
            'å­¦æœ¯ä¸¥è°¨åº¦', 'åˆ›æ–°ç¨‹åº¦', 'å®ç”¨ä»·å€¼', 'å½±å“èŒƒå›´', 'å¯è¯»æ€§',
            'ç»¼åˆè¯„åˆ†_P0', 'æ¨èç­‰çº§_P0'
        ]

        # æ·»åŠ å„åœºæ™¯çš„åˆ—
        for scenario_name in scenarios.keys():
            headers.extend([
                f'ç»¼åˆè¯„åˆ†_{scenario_name}',
                f'æ¨èç­‰çº§_{scenario_name}'
            ])

        writer = csv.writer(f)
        writer.writerow(headers)

        # å†™å…¥æ•°æ®è¡Œ
        for paper in papers:
            row = [
                paper.index,
                paper.title,
                paper.rigor,
                paper.innovation,
                paper.practicality,
                paper.impact,
                paper.readability,
                paper.p0_overall,
                paper.p0_level
            ]

            # è®¡ç®—å„åœºæ™¯è¯„åˆ†
            for scenario_name, scenario_config in scenarios.items():
                scenario_score = calculate_scenario_score(paper, scenario_config['weights'])
                row.extend([
                    scenario_score.overall_score,
                    scenario_score.level
                ])

            writer.writerow(row)

    print(f"[OK] Comparison table generated: {output_path}")


def create_scenario_ranking_tables(papers: List[PaperRecord], config: Dict, output_dir: Path):
    """
    ä¸ºæ¯ä¸ªåœºæ™¯åˆ›å»ºæ’åºè¡¨

    æŒ‰è¯¥åœºæ™¯çš„ç»¼åˆè¯„åˆ†é™åºæ’åˆ—ï¼Œtop 10é«˜äº®
    """
    scenarios = config['scenarios']
    output_dir.mkdir(parents=True, exist_ok=True)

    for scenario_name, scenario_config in scenarios.items():
        # è®¡ç®—æ¯ç¯‡è®ºæ–‡åœ¨è¯¥åœºæ™¯ä¸‹çš„è¯„åˆ†
        scored_papers = []
        for paper in papers:
            scenario_score = calculate_scenario_score(paper, scenario_config['weights'])
            scored_papers.append((paper, scenario_score))

        # æŒ‰è¯„åˆ†é™åºæ’åº
        scored_papers.sort(key=lambda x: x[1].overall_score, reverse=True)

        # ç”Ÿæˆæ–‡ä»¶
        filename = f"æ’åºè¡¨_{scenario_name}.csv"
        filepath = output_dir / filename

        with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
            # è¡¨å¤´ä¸åŸP0ç›¸åŒï¼Œä½†æ·»åŠ è¯¥åœºæ™¯çš„ç»¼åˆè¯„åˆ†å’Œç­‰çº§
            headers = list(papers[0].original_row.keys()) + [
                f'ç»¼åˆè¯„åˆ†_{scenario_name}',
                f'æ¨èç­‰çº§_{scenario_name}',
                'â˜…æ¨èæŒ‡æ•°'  # ç”¨äºæ ‡è®°top 10
            ]

            writer = csv.DictWriter(f, fieldnames=headers)
            writer.writeheader()

            # å†™å…¥æ•°æ®è¡Œ
            for rank, (paper, scenario_score) in enumerate(scored_papers, 1):
                row = paper.original_row.copy()
                row[f'ç»¼åˆè¯„åˆ†_{scenario_name}'] = scenario_score.overall_score
                row[f'æ¨èç­‰çº§_{scenario_name}'] = scenario_score.level

                # æ ‡è®°top 10
                if rank <= 10:
                    row['â˜…æ¨èæŒ‡æ•°'] = f"ğŸ”¥ TOP {rank}"
                else:
                    row['â˜…æ¨èæŒ‡æ•°'] = ""

                writer.writerow(row)

        print(f"[OK] Ranking table generated: {filepath}")
        print(f"   ({scenario_name} - sorted by score, top 10 highlighted)")


def print_summary(papers: List[PaperRecord], config: Dict):
    """æ‰“å°æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "=" * 70)
    print("P1.1 Weight-adaptive Scoring System - Execution Complete")
    print("=" * 70)
    print(f"\nSummary Statistics:")
    print(f"  - Total papers processed: {len(papers)}")
    print(f"  - Weight scenario count: {len(config['scenarios'])}")
    print(f"  - Output plan: Layered output (Plan C)")

    print(f"\nScenario List:")
    for name, scenario_config in config['scenarios'].items():
        print(f"  * {name}")
        print(f"    Description: {scenario_config['description']}")
        print(f"    Weights: rigor={scenario_config['weights']['rigor']}, "
              f"innovation={scenario_config['weights']['innovation']}, "
              f"practicality={scenario_config['weights']['practicality']}, "
              f"impact={scenario_config['weights']['impact']}, "
              f"readability={scenario_config['weights']['readability']}")

    print(f"\nOutput Files:")
    print(f"  [ok] {OUTPUT_DIR / '_all_papers_å¤šåœºæ™¯å¯¹æ¯”è¡¨.csv'}")
    print(f"  [ok] {SCENARIO_RANKINGS_DIR / 'æ’åºè¡¨_åº”ç”¨å¯¼å‘å‹.csv'}")
    print(f"  [ok] {SCENARIO_RANKINGS_DIR / 'æ’åºè¡¨_ç†è®ºçªç ´å‹.csv'}")
    print(f"  [ok] {SCENARIO_RANKINGS_DIR / 'æ’åºè¡¨_ç»¼åˆå‡è¡¡å‹.csv'}")

    print(f"\nUsage Suggestions:")
    print(f"  1. Open '_all_papers_å¤šåœºæ™¯å¯¹æ¯”è¡¨.csv' for in-depth analysis")
    print(f"     - Observe ranking changes for same paper across scenarios")
    print(f"     - Understand impact of weight adjustments on scores")
    print(f"  2. Open ranking tables in 'scenario_rankings/' for quick review")
    print(f"     - For application-oriented papers, open 'æ’åºè¡¨_åº”ç”¨å¯¼å‘å‹.csv'")
    print(f"     - Top 10 marked with 'TOP N' for quick location")
    print(f"  3. Original P0 '_all_papers.csv' unchanged for baseline comparison")

    print("\n" + "=" * 70 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("P1.1 Weight-adaptive Scoring System - Launching")
    print("=" * 70)

    # æ£€æŸ¥ä¾èµ–æ–‡ä»¶
    print("\nChecking dependency files...")
    if not P0_CSV_PATH.exists():
        print(f"ERROR: Cannot find P0 CSV file: {P0_CSV_PATH}")
        return
    if not WEIGHTS_CONFIG_PATH.exists():
        print(f"ERROR: Cannot find weights config file: {WEIGHTS_CONFIG_PATH}")
        return
    print(f"[OK] P0 CSV file: {P0_CSV_PATH}")
    print(f"[OK] Weights config file: {WEIGHTS_CONFIG_PATH}")

    # åŠ è½½æ•°æ®
    print("\nLoading data...")
    config = load_weights_config()
    papers = load_p0_csv()
    print(f"[OK] Successfully loaded {len(papers)} papers")
    print(f"[OK] Successfully loaded {len(config['scenarios'])} weight scenarios")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    print("\nPreparing output directory...")
    SCENARIO_RANKINGS_DIR.mkdir(parents=True, exist_ok=True)
    print(f"[OK] Output directory ready: {OUTPUT_DIR}")

    # ç”Ÿæˆå¤šåœºæ™¯å¯¹æ¯”è¡¨
    print("\nGenerating multi-scenario comparison table...")
    comparison_table_path = OUTPUT_DIR / "_all_papers_å¤šåœºæ™¯å¯¹æ¯”è¡¨.csv"
    create_comparison_table(papers, config, comparison_table_path)

    # ç”ŸæˆæŒ‰åœºæ™¯æ’åºçš„è¡¨
    print("\nGenerating per-scenario ranking tables...")
    create_scenario_ranking_tables(papers, config, SCENARIO_RANKINGS_DIR)

    # æ‰“å°æ±‡æ€»
    print_summary(papers, config)


if __name__ == "__main__":
    main()
